{"cells":[{"cell_type":"markdown","metadata":{"id":"dxk5_UyiyQUn"},"source":["# AML Task 1"]},{"cell_type":"code","execution_count":80,"metadata":{"executionInfo":{"elapsed":1623,"status":"ok","timestamp":1668374369114,"user":{"displayName":"Gabriel Hug","userId":"09098341047120674226"},"user_tz":-60},"id":"sZRoyLOLyZzt"},"outputs":[],"source":["#@title Imports\n","\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error\n","from sklearn.model_selection import train_test_split, cross_validate\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n","from sklearn.linear_model import Ridge, HuberRegressor, RANSACRegressor, LogisticRegression, LinearRegression, BayesianRidge, ARDRegression\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler, Normalizer\n","from sklearn.svm import SVR, LinearSVR\n","from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, IsolationForest, BaggingRegressor, HistGradientBoostingRegressor\n","from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\n","from sklearn.feature_selection import SelectKBest, SelectFromModel, VarianceThreshold\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import AdaBoostRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import StackingRegressor, VotingRegressor\n","from sklearn.linear_model import TheilSenRegressor\n","from xgboost import XGBRegressor\n","from sklearn.cluster import KMeans\n","from sklearn.neural_network import MLPRegressor\n","\n","from sklearn.feature_selection import RFE, RFECV\n","from sklearn.feature_selection import SequentialFeatureSelector\n","\n","from scipy.stats import levene\n","\n","# explicitly require this experimental feature\n","from sklearn.experimental import enable_iterative_imputer  # noqa\n","# now you can import normally from sklearn.impute\n","from sklearn.impute import IterativeImputer"]},{"cell_type":"code","execution_count":81,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5063,"status":"ok","timestamp":1668374375487,"user":{"displayName":"Gabriel Hug","userId":"09098341047120674226"},"user_tz":-60},"id":"XCiDD1cWyhcJ","outputId":"77bc5994-0a1e-42ca-d717-29a19bb32a9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["use_drive = True\n","if use_drive:\n","  PATH = \"drive/My Drive/AML_HS22/task1/\"\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","else:\n","  PATH = \"./\""]},{"cell_type":"code","execution_count":82,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":650,"status":"ok","timestamp":1668374376130,"user":{"displayName":"Gabriel Hug","userId":"09098341047120674226"},"user_tz":-60},"id":"LtfeYwsly0pZ","outputId":"2f02da29-b120-426b-956f-a074fe039523"},"outputs":[{"name":"stdout","output_type":"stream","text":["X shape (1212, 832)\n","y shape (1212, 1)\n","y.ravel() shape (1212,)\n","X_test shape (776, 832)\n"]}],"source":["#@title Load data and generate training validation split\n","\n","def load_data():\n","    X = np.asarray(pd.read_csv(PATH + \"X_train.csv\"))\n","    y = np.asarray(pd.read_csv(PATH + \"y_train.csv\"))\n","    X_test = np.asarray(pd.read_csv(PATH + \"X_test.csv\"))\n","\n","    # delete id column\n","    X = np.delete(X, 0, 1)\n","    y = np.delete(y, 0, 1)\n","    X_test = np.delete(X_test, 0, 1)\n","    print(\"X shape %s\" %str(X.shape))\n","    print(\"y shape %s\" %str(y.shape))\n","    print(\"y.ravel() shape %s\" %str(y.ravel().shape))\n","    print(\"X_test shape %s\" %str(X_test.shape))\n","    \n","    return X, y, X_test\n","\n","\n","X, y, X_test = load_data()\n"]},{"cell_type":"code","execution_count":83,"metadata":{"executionInfo":{"elapsed":78,"status":"ok","timestamp":1668374376137,"user":{"displayName":"Gabriel Hug","userId":"09098341047120674226"},"user_tz":-60},"id":"IUWeva-pn6Sg"},"outputs":[],"source":["#@title Feature Select\n","# # sel = SelectFromModel(RandomForestRegressor(n_estimators=1000, min_samples_leaf=5, n_jobs=-1))\n","# sel = SequentialFeatureSelector(HistGradientBoostingRegressor(random_state=1), n_features_to_select=90) # back to 1000 estimators\n","# # sel = SequentialFeatureSelector(RandomForestRegressor(n_estimators=10, min_samples_leaf=2, n_jobs=-1), n_features_to_select=20) # back to 1000 estimators\n","# # sel = SelectFromModel(RandomForestRegressor(n_estimators=100, min_samples_leaf=5, n_jobs=-1), max_features=25)\n","# sel.fit(X, y.ravel())\n","# X = sel.transform(X)\n","# #X_train = sel.transform(X_train)\n","# #X_val = sel.transform(X_val)\n","# X_test = sel.transform(X_test)\n","# print(\"X shape %s\" %str(X.shape))"]},{"cell_type":"code","execution_count":84,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":82,"status":"ok","timestamp":1668374376144,"user":{"displayName":"Gabriel Hug","userId":"09098341047120674226"},"user_tz":-60},"id":"cpz2JndJ6sJf","outputId":"e15604b8-e2d9-4134-f544-c31d6d4b3934"},"outputs":[{"name":"stdout","output_type":"stream","text":["before impute: [1.77570376e+04            nan 4.10101627e+03 9.29595276e+04\n","            nan 9.98551677e+01 1.00139594e+04 1.08266075e+04\n"," 1.00761016e+04 1.14369696e+01]\n","after impute: [1.77570376e+04 1.08394831e+04 4.10101627e+03 9.29595276e+04\n"," 1.05029940e+02 9.98551677e+01 1.00139594e+04 1.08266075e+04\n"," 1.00761016e+04 1.14369696e+01]\n"]}],"source":["#@title Impute\n","\n","print(\"before impute: %s\" %str(X[1][:10]))\n","\n","imputer = SimpleImputer(strategy='median')\n","# imputer = IterativeImputer()\n","# imputer = KNNImputer(n_neighbors=10, weights='distance')\n","imputer.fit(X)\n","# X_train = imputer.transform(X_train)\n","# X_val = imputer.transform(X_val)\n","X_test = imputer.transform(X_test)\n","X = imputer.transform(X)\n","\n","print(\"after impute: %s\" %str(X[1][:10]))"]},{"cell_type":"code","execution_count":85,"metadata":{"executionInfo":{"elapsed":66,"status":"ok","timestamp":1668374376153,"user":{"displayName":"Gabriel Hug","userId":"09098341047120674226"},"user_tz":-60},"id":"8GwuDlFRACyp"},"outputs":[],"source":["#@title Precompute\n","def simple_precompute(X, y, X_test):\n","\n","  # normalize data\n","  scaler = MinMaxScaler()\n","  scaler.fit(X)\n","  X = scaler.fit_transform(X)\n","  X_test = scaler.fit_transform(X_test)\n","\n","  # remove low variance features\n","  sel = VarianceThreshold(threshold=(0.001))\n","  X = sel.fit_transform(X)\n","  X_test = sel.fit_transform(X_test)\n","\n","  b = []\n","  for i in range(len(X[0])):\n","    stat, p = levene(y[0], X[:, i], center = 'mean')\n","    if p \u003c 0.1:\n","      b.append(True)\n","    else:\n","      b.append(False)\n","  # print(b)\n","  X = X[:, b]\n","  X_test = X_test[:, b]\n","  print(\"X shape %s\" %str(X.shape))\n","  print(\"X_test shape %s\" %str(X_test.shape))\n","\n","  #yX = np.c_[y, X].T\n","  #print(\"yX shape %s\" %str(yX.shape))\n","  #cov = np.absolute(np.cov(yX))\n","  #tr = np.triu(cov, k=0)\n","  #print(\"cov shape %s\" %str(tr.shape))\n","  #print(tr)\n","\n","\n","  return X, y, X_test\n","\n","# simple_precompute(X,y,X_test)\n","# X, y, X_test = simple_precompute(X,y,X_test)"]},{"cell_type":"code","execution_count":86,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":295212,"status":"ok","timestamp":1668374671305,"user":{"displayName":"Gabriel Hug","userId":"09098341047120674226"},"user_tz":-60},"id":"u-IVCLUZcQpp","outputId":"8d2dfc18-5ffd-42ba-dd24-1a77981ed269"},"outputs":[{"name":"stdout","output_type":"stream","text":["X shape (1212, 87)\n"]}],"source":["#@title Feature Select\n","# sel = SelectFromModel(RandomForestRegressor(n_estimators=1000, min_samples_leaf=5, n_jobs=-1))\n","sel = SelectFromModel(RandomForestRegressor(n_estimators=1000, min_samples_leaf=5, n_jobs=-1, random_state=1), threshold=\"mean\") # back to 1000 estimators\n","# sel = RFECV(RandomForestRegressor(n_estimators=25, min_samples_leaf=5, n_jobs=-1, random_state=1), step=10, verbose=10) # back to 1000 estimators\n","# sel = RFE(RandomForestRegressor(n_estimators=10, min_samples_leaf=5, n_jobs=-1, random_state=1), n_features_to_select=90, verbose=10) # back to 1000 estimators\n","# sel = SelectFromModel(XGBRegressor(objective='reg:squarederror', n_estimators=300, random_state=2), max_features=90)\n","# sel = SelectFromModel(RandomForestRegressor(n_estimators=100, min_samples_leaf=5, n_jobs=-1), max_features=25)\n","sel.fit(X, y.ravel())\n","X = sel.transform(X)\n","#X_train = sel.transform(X_train)\n","#X_val = sel.transform(X_val)\n","X_test = sel.transform(X_test)\n","print(\"X shape %s\" %str(X.shape))"]},{"cell_type":"code","execution_count":87,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51,"status":"ok","timestamp":1668374671311,"user":{"displayName":"Gabriel Hug","userId":"09098341047120674226"},"user_tz":-60},"id":"mosedvKdRrZS","outputId":"709cb583-b8c6-44c9-fc62-f98557cbc0dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["X/X_test shape (1988, 87)\n","X shape (1104, 87)\n","y shape (1104, 1)\n"]}],"source":["#@title Outlier Filtering\n","\n","def filter_data(X, y, X_test):\n","  clf = IsolationForest(max_samples=100, random_state=3)\n","  tmp = np.concatenate([X, X_test])\n","  print(\"X/X_test shape %s\" %str(tmp.shape))\n","  clf.fit(tmp)\n","  X_booleans = clf.predict(X)\n","  X_out = []\n","  y_out = []\n","  for i, b in enumerate(X_booleans):\n","    if b == 1:\n","      X_out.append(X[i])\n","      y_out.append(y[i])\n","  X = np.asarray(X_out)\n","  y = np.asarray(y_out)\n","  \n","  print(\"X shape %s\" %str(X.shape))\n","  print(\"y shape %s\" %str(y.shape))\n","\n","  return X, y\n","\n","X, y = filter_data(X, y, X_test)\n","\n"]},{"cell_type":"code","execution_count":88,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41005,"status":"ok","timestamp":1668374712296,"user":{"displayName":"Gabriel Hug","userId":"09098341047120674226"},"user_tz":-60},"id":"l_kAjQ2sdMYM","outputId":"b65cb48a-7d38-444f-f244-f909ea1f429f"},"outputs":[{"name":"stdout","output_type":"stream","text":["RandomForestRegressor: 0.596254\n"]}],"source":["#@title cross validation on random forest to get a feel how well it does\n","et = RandomForestRegressor(n_estimators=300, min_samples_leaf=2, n_jobs=-1, random_state=2)\n","cval = cross_validate(et, X, y, scoring='r2', n_jobs=-1)\n","score = np.average(cval['test_score'])\n","print(\"RandomForestRegressor: %f\" %score)\n","\n","# ablation study\n","# RandomForestRegressor: 0.500328 KNN-imputer, outlier filtering, feature selection\n","# RandomForestRegressor: 0.592574 KNN-imputer, feature selection, outlier filtering\n","# RandomForestRegressor: 0.599434 median imputer, selectfrommodel rf 82 threshold=mean features feature selection, outlier filtering\n","# RandomForestRegressor: 0.514511 median imputer, selectfrommodel rf 82 threshold=mean features feature selection, no outlier filtering\n","# RandomForestRegressor: 0.598990 median imputer, selectfrommodel rf 82 threshold=mean features feature selection, outlier filtering IsolationForest200\n","# RandomForestRegressor: 0.595044 median imputer, selectfrommodel rf 82 threshold=mean features feature selection, outlier filtering IsolationForest200 bootstrap\n","# RandomForestRegressor: 0.596161 median imputer, selectfrommodel rf 60 threshold=1.25*mean features feature selection, outlier filtering\n","# RandomForestRegressor: 0.591115 median imputer, selectfrommodel rf 120 threshold=0.75*mean features feature selection, outlier filtering\n","# RandomForestRegressor: 0.50.... median imputer, selectfrommodel rf 82 threshold=median features feature selection, outlier filtering\n","# RandomForestRegressor: 0.596890 median imputer, selectfrommodel rf 50 features feature selection, outlier filtering\n","# RandomForestRegressor: 0.587515 median imputer, selectfrommodel xgboost 90 features feature selection, outlier filtering\n","# RandomForestRegressor: 0.583112 with RFECV 562 features"]},{"cell_type":"code","execution_count":89,"metadata":{"executionInfo":{"elapsed":587,"status":"ok","timestamp":1668374712298,"user":{"displayName":"Gabriel Hug","userId":"09098341047120674226"},"user_tz":-60},"id":"RFpcoTWNSrWh"},"outputs":[],"source":["#@title Split Data\n","def split_data(X, y):\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)\n","    y_train = y_train.ravel()\n","    y_val = y_val.ravel()\n","    y = y.ravel()\n","\n","    print(\"X shape %s\" %str(X.shape))\n","    print(\"y shape %s\" %str(y.shape))\n","    print(\"X_train shape %s\" %str(X_train.shape))\n","    print(\"y_train shape %s\" %str(y_train.shape))\n","    print(\"X_val shape %s\" %str(X_val.shape))\n","    print(\"y_val shape %s\" %str(y_val.shape))\n","\n","    return X_train, X_val, y_train, y_val, X, y\n","\n","# X_train, X_val, y_train, y_val, X, y = split_data(X, y)"]},{"cell_type":"code","execution_count":90,"metadata":{"executionInfo":{"elapsed":588,"status":"ok","timestamp":1668374712301,"user":{"displayName":"Gabriel Hug","userId":"09098341047120674226"},"user_tz":-60},"id":"Ql2n6Heb4Uu6"},"outputs":[],"source":["#@title model comparison on cross validation\n","def compare_models():\n","  hsgr = HistGradientBoostingRegressor(max_iter=300, random_state=2)\n","  cval = cross_validate(hsgr, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"HistGradientBoostingRegressor: %f\" %score)\n","\n","  xgb = XGBRegressor(n_estimators=300, random_state=2)\n","  cval = cross_validate(xgb, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"XGB: %f\" %score)\n","\n","  rf = RandomForestRegressor(n_estimators=100, max_features=15, min_samples_leaf=2, n_jobs=-1, random_state=2)\n","  cval = cross_validate(rf, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"RandomForest: %f\" %score)\n","\n","  svr = make_pipeline(StandardScaler(), SVR(kernel=\"rbf\", C=1.0, epsilon=0.2))\n","  cval = cross_validate(svr, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"SVR: %f\" %score)\n","\n","  et = ExtraTreesRegressor(n_estimators=100, max_features=15, min_samples_leaf=2, n_jobs=-1, random_state=2)\n","  cval = cross_validate(et, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"ExtraTrees: %f\" %score)\n","\n","  gbr = GradientBoostingRegressor(n_estimators=300, learning_rate=0.1, max_depth=5, random_state=0, loss='squared_error')\n","  cval = cross_validate(gbr, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"GradientBoosting: %f\" %score)\n","\n","  abr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4), n_estimators=300, random_state=0)\n","  cval = cross_validate(abr, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"AdaBoost: %f\" %score)\n","\n","  estimators = [('xgb', xgb), ('gbr', gbr), ('hsgr', hsgr)]\n","  vr = VotingRegressor(estimators)\n","  cval = cross_validate(vr, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"Voting 3: %f\" %score)\n","\n","  sr = StackingRegressor(estimators=estimators, final_estimator=Ridge())\n","  cval = cross_validate(sr, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"Stacking 3: %f\" %score)\n","\n","  estimators = [('rf', rf), ('et', et), ('gbr', gbr), ('abr', abr)]\n","  vr = VotingRegressor(estimators)\n","  cval = cross_validate(vr, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"Voting 4: %f\" %score)\n","\n","  sr = StackingRegressor(estimators=estimators, final_estimator=Ridge())\n","  cval = cross_validate(sr, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"Stacking 4: %f\" %score)\n","\n","  estimators = [('rf', rf), ('et', et), ('xgb', xgb), ('gbr', gbr), ('hsgr', hsgr), ('abr', abr)]\n","  vr = VotingRegressor(estimators)\n","  cval = cross_validate(vr, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"Voting 6: %f\" %score)\n","\n","  sr = StackingRegressor(estimators=estimators, final_estimator=Ridge())\n","  cval = cross_validate(sr, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"Stacking 6: %f\" %score)\n","\n","# compare_models()\n","\n","# simple median imputer\n","# HistGradientBoostingRegressor: 0.628836\n","# XGB: 0.615902\n","# RandomForest: 0.595371\n","# SVR: 0.466644\n","# ExtraTrees: 0.579479\n","# GradientBoosting: 0.605367\n","# AdaBoost: 0.594406\n","# Voting 3: 0.635512\n","# Stacking 3: 0.639591\n","# Voting 4: 0.613904\n","# Stacking 4: 0.628063\n","# Voting 6: 0.631099\n","# Stacking 6: 0.645067"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Ounr5EKp9nIX"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_stacking.py:758: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["[21:25:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n","[21:31:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n","[21:31:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n","[21:31:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n","[21:31:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n","[21:31:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n","[58.0670046  77.8361733  71.99262782 75.80547739 73.58093443 56.13041615\n"," 63.38074222 70.33375836 66.6017805  60.58107339]\n"]}],"source":["#@title Ensemble\n","# fit on entire dataset\n","hsgr = HistGradientBoostingRegressor(max_iter=1000, random_state=2)\n","rf = RandomForestRegressor(n_estimators=1000, min_samples_leaf=2, n_jobs=-1, random_state=2)\n","et = ExtraTreesRegressor(n_estimators=1000, min_samples_leaf=2, n_jobs=-1, random_state=2)\n","gbr = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.1, max_depth=5, random_state=0, loss='squared_error')\n","abr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4), n_estimators=3000, random_state=0)\n","xgb = XGBRegressor(n_estimators=1000, random_state=2)\n","estimators = [('rf', rf), ('et', et), ('xgb', xgb), ('gbr', gbr), ('hsgr', hsgr), ('abr', abr)]\n","sr = StackingRegressor(estimators=estimators, final_estimator=Ridge())\n","clf = sr.fit(X, y)\n","\n","y_test = sr.predict(X_test)\n","print(y_test[:10])\n","\n","# for ridge\n","# ids = np.asarray(list(range(len(y_test)))).reshape((-1, 1)).astype(int)\n","# print(ids[:10])\n","# output_arr = np.hstack((ids, y_test))\n","\n","# for SVR\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"opO2X9Ei2hDL"},"outputs":[{"name":"stdout","output_type":"stream","text":["[58.0670046  77.8361733  71.99262782 75.80547739 73.58093443 56.13041615\n"," 63.38074222 70.33375836 66.6017805  60.58107339]\n","[[ 0.         58.0670046 ]\n"," [ 1.         77.8361733 ]\n"," [ 2.         71.99262782]\n"," [ 3.         75.80547739]\n"," [ 4.         73.58093443]\n"," [ 5.         56.13041615]\n"," [ 6.         63.38074222]\n"," [ 7.         70.33375836]\n"," [ 8.         66.6017805 ]\n"," [ 9.         60.58107339]]\n"]}],"source":["#@title Output\n","y_test = clf.predict(X_test)\n","print(y_test[:10])\n","ids = np.asarray(list(range(len(y_test)))).astype(int)\n","output_arr = np.column_stack((ids, y_test))\n","print(output_arr[:10])\n","np.savetxt(PATH + 'y_test.csv', output_arr, delimiter=',', header=\"id,y\", comments='', fmt=[\"%d\",\"%f\"])"]},{"cell_type":"markdown","metadata":{"id":"qDpvllpWpFve"},"source":["# Other Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SEBdkIzjlJ9o"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_from_model.py:267: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  self.estimator_.fit(X, y, **fit_params)\n"]},{"name":"stdout","output_type":"stream","text":["X shape (1104, 14)\n","RandomForest 30: 0.573641\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_from_model.py:267: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  self.estimator_.fit(X, y, **fit_params)\n"]},{"name":"stdout","output_type":"stream","text":["X shape (1104, 14)\n","RandomForest 35: 0.573641\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_from_model.py:267: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  self.estimator_.fit(X, y, **fit_params)\n"]},{"name":"stdout","output_type":"stream","text":["X shape (1104, 14)\n","RandomForest 45: 0.573641\n"]}],"source":["feature_count = [30, 35, 45]\n","for fcount in feature_count:\n","  # sel = SelectFromModel(RandomForestRegressor(n_estimators=1000, min_samples_leaf=5, n_jobs=-1))\n","  sel = SelectFromModel(RandomForestRegressor(n_estimators=300, min_samples_leaf=5, n_jobs=-1, random_state=1), max_features=fcount) # back to 1000 estimators\n","  # sel = SelectFromModel(RandomForestRegressor(n_estimators=100, min_samples_leaf=5, n_jobs=-1), max_features=25)\n","  sel.fit(X, y)\n","  X_p = sel.transform(X)\n","  print(\"X shape %s\" %str(X_p.shape))\n","\n","  rf = RandomForestRegressor(n_estimators=300, min_samples_leaf=2, n_jobs=-1, random_state=2)\n","  cval = cross_validate(rf, X_p, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"RandomForest %d: %f\" %(fcount, score))\n","\n","# RandomForest 10: 0.473281\n","# RandomForest 20: 0.502336\n","# RandomForest 30: 0.514247\n","# RandomForest 35: 0.521251\n","# RandomForest 40: 0.523973\n","# RandomForest 45: 0.524660 \u003c\u003c\u003c\n","\n","\n","# it seems 45 features overfits the training set, but generalizes not so well to the test set on the project server\n","\n","# RandomForest 50: 0.521228\n","# RandomForest 55: 0.522183\n","# RandomForest 60: 0.522521\n","# RandomForest 65: 0.520342\n","# RandomForest 70: 0.519508\n","# RandomForest 80: 0.515016\n","# RandomForest 100 (only got 82): 0.514511"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wg0bLCLaoiVE"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_from_model.py:267: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  self.estimator_.fit(X, y, **fit_params)\n"]},{"name":"stdout","output_type":"stream","text":["X shape (1104, 14)\n"]},{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-94-d0bb56ea9486\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0msel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectFromModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcount\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# back to 1000 estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# sel = SelectFromModel(RandomForestRegressor(n_estimators=100, min_samples_leaf=5, n_jobs=-1), max_features=25)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 12\u001b[0;31m   \u001b[0msel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mX_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X shape %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_from_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    258\u001b[0m                 raise ValueError(\n\u001b[1;32m    259\u001b[0m                     \"'max_features' should be 0 and {} features.Got {} instead.\".format(\n\u001b[0;32m--\u003e 260\u001b[0;31m                         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m                     )\n\u001b[1;32m    262\u001b[0m                 )\n","\u001b[0;31mValueError\u001b[0m: 'max_features' should be 0 and 14 features.Got 20 instead."]}],"source":["feature_count = [40, 60, 80, 90, 100, 150]\n","for fcount in feature_count:\n","  # sel = SelectFromModel(RandomForestRegressor(n_estimators=1000, min_samples_leaf=5, n_jobs=-1))\n","  sel = SelectFromModel(RandomForestRegressor(n_estimators=300, min_samples_leaf=5, n_jobs=-1, random_state=1), max_features=fcount) # back to 1000 estimators\n","  # sel = SelectFromModel(RandomForestRegressor(n_estimators=100, min_samples_leaf=5, n_jobs=-1), max_features=25)\n","  sel.fit(X, y)\n","  X_p = sel.transform(X)\n","  print(\"X shape %s\" %str(X_p.shape))\n","\n","  sel = SelectFromModel(RandomForestRegressor(n_estimators=300, min_samples_leaf=5, n_jobs=-1, random_state=1), max_features=fcount//2) # back to 1000 estimators\n","  # sel = SelectFromModel(RandomForestRegressor(n_estimators=100, min_samples_leaf=5, n_jobs=-1), max_features=25)\n","  sel.fit(X_p, y)\n","  X_p = sel.transform(X_p)\n","  print(\"X shape %s\" %str(X_p.shape))\n","\n","  rf = RandomForestRegressor(n_estimators=300, min_samples_leaf=2, n_jobs=-1, random_state=2)\n","  cval = cross_validate(rf, X_p, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"RandomForest %d: %f\" %(fcount, score))\n","\n","# X shape (1212, 40)\n","# X shape (1212, 7)\n","# RandomForest 40: 0.454094\n","# X shape (1212, 60)\n","# X shape (1212, 11)\n","# RandomForest 60: 0.477203\n","# X shape (1212, 80)\n","# X shape (1212, 14)\n","# RandomForest 80: 0.487155\n","# X shape (1212, 82)\n","# X shape (1212, 14)\n","# RandomForest 90: 0.487155"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TgZAD3IHVF3Y"},"outputs":[],"source":["'''\n","rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=2, n_jobs=-1, random_state=2)\n","rf.fit(X_train, y_train)\n","y_pred = rf.predict(X_val)\n","print(f'\u003e\u003e\u003e r2_score: {r2_score(y_val, y_pred)} \u003c\u003c\u003c')\n","\n","clf = IsolationForest(max_samples=100, random_state=3)\n","clf.fit(X_train)\n","X_train_booleans = clf.predict(X_train)\n","print(np.asarray(X_train_booleans))\n","\n","X_train_prime = []\n","y_train_prime = []\n","for i, b in enumerate(X_train_booleans):\n","  if b == 1:\n","    X_train_prime.append(X_train[i])\n","    y_train_prime.append(y_train[i])\n","X_train = np.asarray(X_train_prime)\n","y_train = np.asarray(y_train_prime)\n","print(\"X_train shape %s\" %str(X_train.shape))\n","\n","rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=2, n_jobs=-1, random_state=2)\n","rf.fit(X_train, y_train)\n","y_pred = rf.predict(X_val)\n","print(f'\u003e\u003e\u003e r2_score: {r2_score(y_val, y_pred)} \u003c\u003c\u003c')\n","'''\n","\n","# output\n","# \u003e\u003e\u003e r2_score: 0.47233612984084306 \u003c\u003c\u003c\n","# X_train shape (969, 87)\n","# X_train shape (884, 87)\n","# \u003e\u003e\u003e r2_score: 0.40993444633687215 \u003c\u003c\u003c\n","\n","# conclusion: removing outliers from training set does not improve performance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hFRSC_3MBlLL"},"outputs":[],"source":["# est = TheilSenRegressor()\n","# cval = cross_validate(est, X, y, scoring='r2', n_jobs=-1)\n","# score = np.average(cval['test_score'])\n","# print(\"Est: %f\" %score)\n","# Est: 0.319679"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Z0Obk-8anBOA"},"outputs":[],"source":["max_features = [5, 7, 9, 12, 14, 15, 16, 18, 20, 25, 30, 35, 40, 50, 70]\n","for n in max_features:\n","  rf = ExtraTreesRegressor(n_estimators=300, max_features=n, min_samples_leaf=2, n_jobs=-1, random_state=2)\n","  cval = cross_validate(rf, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"ExtraTreesRegressor max_features %d: %f\" %(n, score))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"K620WTYkluVZ"},"outputs":[],"source":["max_features = [5, 7, 9, 12, 14, 15, 16, 18, 20, 25, 30, 35, 40, 50, 70]\n","for n in max_features:\n","  rf = RandomForestRegressor(n_estimators=300, max_features=n, min_samples_leaf=2, n_jobs=-1, random_state=2)\n","  cval = cross_validate(rf, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"RandomForest max_features %d: %f\" %(n, score))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9Nsd-CQ6k3fR"},"outputs":[],"source":["max_samples = [50, 100, 200, 300, 500, 969]\n","for n in max_samples:\n","  rf = RandomForestRegressor(n_estimators=300, max_samples=n, min_samples_leaf=2, n_jobs=-1, random_state=2)\n","  cval = cross_validate(rf, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"RandomForest max_samples %d: %f\" %(n, score))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"p8-RwTRMivuh"},"outputs":[],"source":["ntrees = [50, 100, 200, 300, 500, 1000, 2000]\n","for n in ntrees:\n","  rf = RandomForestRegressor(n_estimators=n, min_samples_leaf=2, n_jobs=-1, random_state=2)\n","  cval = cross_validate(rf, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"RandomForest ntrees %d: %f\" %(n, score))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CcHtxRJ3hI9g"},"outputs":[],"source":["max_depth = [2, 3, 4, 5, 7, 9, 13, 17, 25, 100]\n","for d in max_depth:\n","  rf = RandomForestRegressor(n_estimators=300, max_depth=d, min_samples_leaf=2, n_jobs=-1, random_state=2)\n","  cval = cross_validate(rf, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"RandomForest max_depth %d: %f\" %(d, score))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CXdtX3I7iQ5J"},"outputs":[],"source":["max_depth = [2, 3, 4, 5, 7, 9, 13, 17, 25, 50, 100]\n","for d in max_depth:\n","  rf = ExtraTreesRegressor(n_estimators=300, max_depth=d, min_samples_leaf=2, n_jobs=-1, random_state=2)\n","  cval = cross_validate(rf, X, y, scoring='r2', n_jobs=-1)\n","  score = np.average(cval['test_score'])\n","  print(\"RandomForest max_depth %d: %f\" %(d, score))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yX1OiYoX-WBb"},"outputs":[],"source":["def get_best():\n","  msl = [2, 3, 4]\n","  # msl = [5]\n","  scores = []\n","  for n in msl:\n","    print(f\"min_samples_leaf={n}\")\n","    # print(\"RandomForest\")\n","    # est = RandomForestRegressor(n_estimators=100, min_samples_leaf=n, n_jobs=-1)\n","    # cval = cross_validate(est, X, y, scoring='r2', n_jobs=-1)\n","    # score = np.average(cval['test_score'])\n","    # print(score)\n","    # scores.append(score)\n","\n","    print(\"ExtraTrees\")\n","    est = ExtraTreesRegressor(n_estimators=100, min_samples_leaf=n, n_jobs=-1)\n","    cval = cross_validate(est, X, y, scoring='r2', n_jobs=-1)\n","    score = np.average(cval['test_score'])\n","    print(score)\n","    scores.append(score)\n","\n","    # print(\"RandomForest\")\n","    # est = RandomForestRegressor(n_estimators=1000, min_samples_leaf=n, n_jobs=-1)\n","    # cval = cross_validate(est, X, y, scoring='r2', n_jobs=-1)\n","    # score = np.average(cval['test_score'])\n","    # print(score)\n","    # scores.append(score)\n","\n","    # print(\"ExtraTrees\")\n","    # est = ExtraTreesRegressor(n_estimators=1000, min_samples_leaf=n, n_jobs=-1)\n","    # cval = cross_validate(est, X, y, scoring='r2', n_jobs=-1)\n","    # score = np.average(cval['test_score'])\n","    # print(score)\n","    # scores.append(score)\n","\n","    # min_samples_leaf=5\n","    # ExtraTrees\n","    # 0.5167717523131301\n","\n","  print(scores)\n","  ind = np.argmax(scores)\n","  print('min_samples_leaf: ' + str(msl[ind]))\n","  print('scores: ' + str(scores[ind]))\n","\n","  clf = RandomForestRegressor(min_samples_leaf=msl[ind], n_jobs=-1)\n","\n","  return clf\n","\n","get_best()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Vr72mt1yeavv"},"outputs":[],"source":["est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=0, loss='squared_error')\n","cval = cross_validate(est, X, y, scoring='r2', n_jobs=-1)\n","score = np.average(cval['test_score'])\n","print(score)\n","# # 0.45947468851701745"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BbVy3lY_fR8b"},"outputs":[],"source":["est = AdaBoostRegressor(\n","    DecisionTreeRegressor(max_depth=4), n_estimators=300, random_state=0 # 0.45931666632586976\n",")\n","cval = cross_validate(est, X, y, scoring='r2', n_jobs=-1)\n","score = np.average(cval['test_score'])\n","print(score)"]},{"cell_type":"markdown","metadata":{"id":"8-xW9GyZ77wQ"},"source":["## Model fitting"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SfaxU5LA73Ol"},"outputs":[],"source":["def fit_classifier(X_data, y_data):\n","  print(\"fit_classifier\")\n","  # clf = Ridge(alpha=1000000.0)\n","  # clf = LinearRegression()\n","  # clf = LogisticRegression()\n","  # clf = RANSACRegressor(random_state=0)\n","  # clf = SVR(kernel=\"rbf\", C=1.0, epsilon=0.2)\n","  # clf = make_pipeline(Normalizer(), SVR(C=1.0, epsilon=0.2)) # did worse than default SVR\n","  # clf = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2)) # did worse than default SVR\n","  # clf = make_pipeline(PolynomialFeatures(2), Ridge(alpha=1.0))\n","  # clf = Pipeline([\n","  #   ('feature_selection', SelectFromModel(LinearSVR())),\n","  #   ('classification', RandomForestClassifier())\n","  #   ])\n","  #clf = RandomForestClassifier(n_estimators=1000, random_state=1, max_features=8)\n","  clf = MLPRegressor(hidden_layer_sizes=(10,1000,1))\n","  # clf = ExtraTreesRegressor(n_estimators=1000, min_samples_leaf=5, n_jobs=-1)\n","\n","  clf.fit(X_data, y_data.ravel())\n","  return clf\n","\n","clf = fit_classifier(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"RStEJUyK82dD"},"source":["## Model evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"M0W6Khvc840y"},"outputs":[],"source":["def print_stats(y_val, y_val_pred):\n","  print(f'\u003e\u003e\u003e r2_score: {r2_score(y_val, y_val_pred)} \u003c\u003c\u003c')\n","  # print(f'mean_squared_error: {mean_squared_error(y_val, y_val_pred)}')\n","  # print(f'root_mean_squared_error: {mean_squared_error(y_val, y_val_pred, squared=False)}')\n","  # print(f'mean_absolute_error: {mean_absolute_error(y_val, y_val_pred)}')\n","  # print(f'median_absolute_error: {median_absolute_error(y_val, y_val_pred)}')\n","\n","y_pred = clf.predict(X_val)\n","print_stats(y_val, y_pred)\n","# BaggingRegressor          0.2970000173416478\n","# AdaBoostRegressor         0.3079485558905629\n","# ExtraTreesRegressor       0.37519049014637373\n","# GradientBoostingRegressor 0.416384019256161 \n","# RandomForestRegressor     0.39808813525261155 \n","# XGBRegressor (default)    0.4148294673073579 \n","# MLPRegressor (10,1)     -49.35169434996628\n","  "]},{"cell_type":"markdown","metadata":{"id":"3eOoAIeh84Em"},"source":["## Generate test predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-POF6BZipO7o"},"outputs":[],"source":["# et = ExtraTreesRegressor(n_estimators=100, max_features=20, min_samples_leaf=2, n_jobs=-1, random_state=2)\n","# cval = cross_validate(et, X, y, scoring='r2', n_jobs=-1)\n","# score = np.average(cval['test_score'])\n","# print(\"ExtraTrees: %f\" %score)\n","\n","# ExtraTrees w max_features=87: 0.529854\n","# ExtraTrees w max_features=20: 0.511661\n","\n","clf = MLPRegressor(hidden_layer_sizes=(10,1,), activation = 'relu', random_state=2)\n","cval = cross_validate(clf, X, y, scoring='r2', n_jobs=-1)\n","score = np.average(cval['test_score'])\n","print(score)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HsxjXiCcwHym"},"outputs":[],"source":["from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}